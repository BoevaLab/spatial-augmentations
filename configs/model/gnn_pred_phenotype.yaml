_target_: src.models.bgrl_phenotype_module.BGRLPhenotypeLitModule

mode: "finetuning"

net:
  _target_: src.models.components.gnn.GNN_pred
  num_layer: ${subgraph_size}
  num_node_type: 30
  num_feat: 76
  emb_dim: 256
  num_additional_feat: 0
  num_graph_tasks: 1
  node_embedding_output: "last"
  drop_ratio: 0.25
  graph_pooling: "max"
  gnn_type: "gin"

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0005
  weight_decay: 1e-4

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: 50000
  eta_min: 1e-5

compile: false # compile model for faster training with pytorch 2.0
augmentation_mode: "baseline" # augmetation mode, either baseline or advanced
augmentation_list1: [] # list of augmentations to apply for the first view (only necessary for advanced mode)
augmentation_list2: [] # list of augmentations to apply for the second view (only necessary for advanced mode)
mm: 0.99 # momentum for moving average of target encoder
warmup_steps: 2000 # number of warmup steps
total_steps: 50000 # total number of training steps
drop_edge_p1: 0.3 # drop edge probability for first augmentation
drop_edge_p2: 0.3 # drop edge probability for second augmentation
drop_feat_p1: 0.3 # drop feature probability for first augmentation
drop_feat_p2: 0.3 # drop feature probability for second augmentation
seed: ${seed} # random seed
ckpt_file: null # path to the checkpoint file for pretraining
