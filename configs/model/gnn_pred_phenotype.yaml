_target_: src.models.bgrl_phenotype_module.BGRLPhenotypeLitModule

mode: "finetuning"

net:
  _target_: src.models.components.gnn.GNN_pred
  num_layer: 3
  num_node_type: 30
  num_feat: 76
  emb_dim: 256
  num_additional_feat: 0
  num_graph_tasks: 1
  node_embedding_output: "last"
  drop_ratio: 0.38
  graph_pooling: "max"
  gnn_type: "gin"

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 9.9e-05
  weight_decay: 0.06

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: 20000
  eta_min: 1e-7

test_thresh: None # threshold for test set
compile: false # compile model for faster training with pytorch 2.0
augmentation_mode: "baseline" # augmetation mode, either baseline or advanced
augmentation_list1: [] # list of augmentations to apply for the first view (only necessary for advanced mode)
augmentation_list2: [] # list of augmentations to apply for the second view (only necessary for advanced mode)
mm: 0.96 # momentum for moving average of target encoder
warmup_steps: 2000 # number of warmup steps
total_steps: 50000 # total number of training steps
drop_edge_p1: 0.3 # drop edge probability for first augmentation
drop_edge_p2: 0.3 # drop edge probability for second augmentation
drop_feat_p1: 0.3 # drop feature probability for first augmentation
drop_feat_p2: 0.3 # drop feature probability for second augmentation
seed: ${seed} # random seed
ckpt_file: null # path to the checkpoint file for pretraining
